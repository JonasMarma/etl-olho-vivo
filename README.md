# ğŸš ETL - Dados da API Olho Vivo (SPTrans)

Este projeto implementa um ETL bÃ¡sico sobre os dados pÃºblicos da **API Olho Vivo**, disponibilizados pela SPTrans, com foco em dados de posiÃ§Ã£o e velocidade de Ã´nibus da cidade de SÃ£o Paulo. Visando fornecer datasets confiÃ¡veis e acessÃ­veis para anÃ¡lises sobre mobilidade urbana e sobretudo, um histÃ³rico de posiÃ§Ãµes para analistas e cientistas de dados.

## ğŸ“‚ Estrutura do Projeto

O projeto estÃ¡ dividido em trÃªs scripts principais:

### 1. `get-bus-data.py` - IngestÃ£o Bruta
ResponsÃ¡vel por consumir os dados diretamente da API Olho Vivo e armazenÃ¡-los em formato JSON para posterior transformaÃ§Ã£o.

### 2. `etl-olho-vivo-ingestao-posicoes.py` - TransformaÃ§Ã£o e Armazenamento
Realiza o **flattening** dos dados JSON brutos, padronizando-os em **formato Parquet** e enviando-os para um **bucket S3**. Ã‰ executado diariamente Ã s **6:00 AM (GMT-3)** via **AWS EventBridge**.

### 3. `etl-olho-vivo-velocidades-medias.py` - AgregaÃ§Ã£o e GeraÃ§Ã£o de Datasets
Consolida os dados processados anteriormente, gerando arquivos **CSV** com agregaÃ§Ãµes Ãºteis para anÃ¡lise, como:

- Velocidade mÃ©dia por trajeto
- Pontos de lentidÃ£o identificados
- LocalizaÃ§Ã£o de veÃ­culos com/sem acessibilidade a cada 30 minutos

Executado automaticamente Ã s **6:30 AM (GMT-3)** via EventBridge.

## ğŸ§¼ Processos de Limpeza Aplicados na `etl-olho-vivo-velocidades-medias.py

- ğŸ”» **Intervalos > 10 minutos** entre aquisiÃ§Ãµes sÃ£o descartados para manter a precisÃ£o
- ğŸ“ **DistÃ¢ncias arredondadas** a duas casas decimais (centÃ­metros - mais do que o suficiente)
- ğŸš« Tuplas com **posiÃ§Ã£o anterior nula** sÃ£o ignoradas
- ğŸš€ **Velocidades > 120 km/h** sÃ£o desconsideradas como anomalias

## ğŸ“Š Resultados e Produtos Gerados

- Datasets limpos e agregados prontos para anÃ¡lise
- TrÃªs principais tabelas derivadas:
  - Velocidades mÃ©dias
  - Pontos de lentidÃ£o
  - LocalizaÃ§Ã£o com acessibilidade

## â˜ï¸ Armazenamento e Acesso

- Os dados completos estÃ£o armazenados na **AWS S3 Glacier**, por questÃµes de custo.
- Amostras estÃ£o disponÃ­veis no **Kaggle**:
  ğŸ‘‰ [kaggle.com/datasets/jonasmarma/samples-bus-data-from-sao-paulo](https://www.kaggle.com/datasets/jonasmarma/samples-bus-data-from-sao-paulo)

Para obter acesso completo aos dados, entre em contato via email: jonas.marma@gmail.com

## ğŸ“ˆ PrÃ³ximos Passos

- Melhorar a **documentaÃ§Ã£o** e acessibilidade dos dados
- Criar um **template de deploy rÃ¡pido via CloudFormation**
- Desenvolver **dashboards no Looker Studio** para visualizaÃ§Ã£o interativa
- Incentivar **colaboraÃ§Ãµes e feedbacks** da comunidade

## ğŸ§  Insights e ConsideraÃ§Ãµes

Este projeto demonstrou que Ã© possÃ­vel construir um pipeline robusto, confiÃ¡vel e de baixo custo para anÃ¡lise de dados de mobilidade urbana, mesmo em ambientes com recursos financeiros limitados.

As maiores dificuldades foram:
- Balancear custo e performance
- Garantir a **qualidade dos dados** sem sacrificar a **escalabilidade**
- Armazenar grandes volumes sem comprometer acessibilidade


---

**Contato:**  
Para dÃºvidas, colaboraÃ§Ãµes ou acesso completo aos dados, abra uma issue ou envie um pull request.

